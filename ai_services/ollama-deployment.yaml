apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-deployment
  labels:
    app: ollama
spec:
  replicas: 1  # Number of replicas for load balancing
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      runtimeClassName: nvidia
      containers:
      - name: ollama
        image: nvantu/drafas-ollama:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 11434  # Port where Ollama listens for requests
        resources:
          limits:
            nvidia.com/gpu: 1  # Use 1 GPU, remove this line if using CPU-only
            cpu: 3
          requests:
            nvidia.com/gpu: 1
            cpu: 1
        env:
        - name: OLLAMA_NUM_PARALLEL
          value: "1"
        - name: OLLAMA_MAX_QUEUE
          value: "5"
        - name: OLLAMA_KEEP_ALIVE
          value: "-1" # keep forever
        # readinessProbe:
        #   httpGet:
        #     path: /
        #     port: 11434
        #   initialDelaySeconds: 3
        #   periodSeconds: 1
        #   timeoutSeconds: 5
        #   successThreshold: 1
        #   failureThreshold: 60
        startupProbe:
          # httpGet:
          #   path: /
          #   port: 11434
          exec:
            command:
            - /bin/sh
            - -c
            - |
              curl -X POST http://localhost:11434/api/generate -d '{"model":"llama3.2:1b-instruct-q4_K_M","options":{"num_gpu":99}}'
          failureThreshold: 60
          periodSeconds: 6
---
apiVersion: v1
kind: Service
metadata:
  name: ollama-service
spec:
  selector:
    app: ollama
  ports:
  - name: http
    protocol: TCP
    port: 11434          # External port
    targetPort: 11434  # The port on the Ollama container
    nodePort: 30434
  type: LoadBalancer  # Use NodePort if not on a cloud provider
---
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: local-ratelimit-ollama
  namespace: istio-system
spec:
  workloadSelector:
    labels:
      app: ollama
  configPatches:
    - applyTo: HTTP_FILTER
      match:
        context: SIDECAR_INBOUND
        listener:
          filterChain:
            filter:
              name: "envoy.filters.network.http_connection_manager"
      patch:
        operation: INSERT_BEFORE
        value:
          name: envoy.filters.http.local_ratelimit
          typed_config:
            "@type": type.googleapis.com/udpa.type.v1.TypedStruct
            type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit
            value:
              stat_prefix: http_local_rate_limiter
    - applyTo: HTTP_ROUTE
      match:
        context: SIDECAR_INBOUND
        routeConfiguration:
          vhost:
            name: "inbound|http|11434"
            route:
              action: ANY
      patch:
        operation: MERGE
        value:
          # match: {prefix: "/api/generatex"}
          typed_per_filter_config:
            envoy.filters.http.local_ratelimit:
              "@type": type.googleapis.com/udpa.type.v1.TypedStruct
              type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit
              value:
                stat_prefix: http_local_rate_limiter
                token_bucket:
                  max_tokens: 7
                  tokens_per_fill: 7
                  fill_interval: 2s
                filter_enabled:
                  runtime_key: local_rate_limit_enabled
                  default_value:
                    numerator: 100
                    denominator: HUNDRED
                filter_enforced:
                  runtime_key: local_rate_limit_enforced
                  default_value:
                    numerator: 100
                    denominator: HUNDRED
                response_headers_to_add:
                  - append: false
                    header:
                      key: x-local-rate-limit
                      value: 'true'

